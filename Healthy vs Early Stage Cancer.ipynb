{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Healthy vs Early Stage Cancer (comparison method)\n",
    "\n",
    "### Preprocessing for early stage cancer vs screening stage cancer\n",
    "\n",
    "### Import Libraries\n",
    "import os # importing operating system library\n",
    "import numpy as np # importing numpy library\n",
    "import pandas as pd # importing pandas library\n",
    "import matplotlib as mp # importing matplotlib library\n",
    "import statsmodels.api as sm # importing statsmodels library\n",
    "\n",
    "### Access directory\n",
    "os.getcwd()\n",
    "os.chdir('/Users/tech26/Desktop/NUS/ACADEMICS/Y1S2/IT1244/Project/Code/IT1244-Final-Project') # change directory as neccesary\n",
    "\n",
    "### Read Data / Visualise\n",
    "trainp_data = pd.read_csv('Train_Set_edit.csv') # access training data\n",
    "trainp_data.head(5) # information on first five data points\n",
    "print(trainp_data.shape) # dimensions of data set\n",
    "trainp_data.describe() # statistical information on data set\n",
    "trainp_data.isna().sum() # check how many data points missing\n",
    "print(trainp_data.iloc[:, -1]) # view response variable (status of cancer)\n",
    "print(trainp_data['class_label'].value_counts()) # distribution of response variable (status of cancer)\n",
    "\n",
    "### Remove classes that are not either healthy or screening stage cancer\n",
    "trainp_data = trainp_data[(trainp_data['class_label'] == 'healthy') | (trainp_data['class_label'] == 'early stage cancer')] # remove non healthy and non early stage cancer classes\n",
    "trainp_data.reset_index(drop=True, inplace=True)\n",
    "print(\"Shape of filtered data:\", trainp_data.shape)\n",
    "\n",
    "### Initialise x and y variables \n",
    "xvals = trainp_data.iloc[:, 0:350] # access x values\n",
    "yvals = trainp_data.iloc[:,350] # access y values\n",
    "\n",
    "### Standardising x values\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "std_x = StandardScaler() # initialise StandardScaler\n",
    "x_std = std_x.fit_transform(xvals) # standardise x values\n",
    "\n",
    "### Techniques to remove regressors\n",
    "\n",
    "### Perform logistic regression to identify insignifcant regressors based on p value\n",
    "yvals_binary = yvals.map({'early stage cancer': 1, 'healthy': 0}) # Convert \"screening stage cancer\" to 1 and \"healthy\" to 0\n",
    "print(yvals_binary.unique()) # double confirm correct conversion of the y values \n",
    "logit_model = sm.Logit(yvals_binary, x_std) # carry out logistic regression\n",
    "result = logit_model.fit()\n",
    "print(result.summary()) # summary of model\n",
    "p_values = result.pvalues # obtain p values of regressor features in model\n",
    "significant_features_1 = list(p_values[p_values < 0.05].index.tolist()) # Filter out significant features with p-value < 0.05\n",
    "insignificant_features_1 = list(p_values[p_values > 0.05].index.tolist()) # Filter out insignificant features with p-value > 0.05\n",
    "\n",
    "### Perform L1 (lasso) regularisation to logistic regression model to idenitify insignifcant regressors with coefficients zero\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear')  # specify solver for L1 regularization\n",
    "log_reg_l1.fit(x_std, yvals_binary)  # train model\n",
    "significant_features_2 = list(xvals.columns[log_reg_l1.coef_[0] != 0]) # check if there are any signifcant features identified\n",
    "insignificant_features_2 = list(xvals.columns[log_reg_l1.coef_[0] == 0]) # identify insignifcant features\n",
    "\n",
    "### Perform principal component analysis to identify insignifcant regressors that contrivbute little to variance\n",
    "from sklearn.decomposition import PCA \n",
    "pca = PCA() # initialise PCA with default parameters\n",
    "x_pca = pca.fit_transform(x_std) # compute principal components and transforms data into new feature space\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  # Get explained variance ratio for each component\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio) # sum computed variance for certain number of components\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cumulative_explained_variance_ratio) # plot cumulative explained variance ratio\n",
    "plt.grid(True) # add grid to the plot\n",
    "plt.show() # show graph\n",
    "\n",
    "chosen_num_components = 25  # plateu cannot be read accurately so limit x axis to [0,25] and plot again\n",
    "plt.plot(range(1, chosen_num_components + 1), cumulative_explained_variance_ratio[:chosen_num_components])\n",
    "plt.grid(True) # add grid to the plot\n",
    "plt.show() # plateu can be read \n",
    "\n",
    "chosen_num_components = 3 # there is little to no change in cumulative variance when more than 3 principal components are involved in calculations\n",
    "pca = PCA(n_components = chosen_num_components) # initialise new pca with no of components = 3\n",
    "x_pca = pca.fit_transform(x_std) # compute principal components and transforms data into new feature space\n",
    "\n",
    "original_features = xvals.columns # all features\n",
    "significant_feature_indices = np.where(cumulative_explained_variance_ratio <= 0.95)[0] # identify festures that are responsible for 95 oercent of variance\n",
    "significant_features_3 = list(original_features[significant_feature_indices])\n",
    "insignificant_feature_indices = np.where(cumulative_explained_variance_ratio > 0.95)[0]\n",
    "insignificant_features_3 = list(original_features[insignificant_feature_indices])\n",
    "\n",
    "### Perform correlation analysis to identify insignifcant regressors that are are highly correlated\n",
    "correlation_matrix = xvals.corr() # find correlation coefficients between each pair of variables\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool)) # mask to consider only the upper triangular matrix and ignore correlation between the same variable. Output is true and false\n",
    "correlation_matrix = correlation_matrix.mask(mask) # apply mask to correlation matrix\n",
    "significant_features_4 = list(xvals.columns)\n",
    "insignificant_features_4 = []\n",
    " \n",
    "for i in range(len(correlation_matrix.columns)): # Loop through the columnns\n",
    "    for j in range(i): # Loop through the rows\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:  # Adjust the threshold as needed. Set 0.8 here\n",
    "            colname_i = correlation_matrix.columns[i] # first compared feature\n",
    "            colname_j = correlation_matrix.columns[j] # second compared feature\n",
    "            if colname_i in significant_features_4:\n",
    "                significant_features_4.remove(colname_i) # update the significant features\n",
    "            if colname_i not in insignificant_features_4:\n",
    "                insignificant_features_4.append(colname_i) # update the siugnifcant features\n",
    "\n",
    "### Decide which features to remove\n",
    "\n",
    "### check lengths of features to be removed\n",
    "removables = {}\n",
    "for feature in insignificant_features_1:\n",
    "    if feature not in removables:\n",
    "        removables[feature] = 0\n",
    "    removables[feature] += 1\n",
    "\n",
    "for feature in insignificant_features_2:\n",
    "    if feature not in removables:\n",
    "        removables[feature] = 0\n",
    "    removables[feature] += 1\n",
    "\n",
    "for feature in insignificant_features_3:\n",
    "    if feature not in removables:\n",
    "        removables[feature] = 0\n",
    "    removables[feature] += 1\n",
    "\n",
    "for feature in insignificant_features_4:\n",
    "    if feature not in removables:\n",
    "        removables[feature] = 0\n",
    "    removables[feature] += 1\n",
    "\n",
    "len(xvals.columns) # no of features in\\ dataset \n",
    "\n",
    "drops_2 = []\n",
    "for feature in removables:\n",
    "    if removables[feature] >= 2:\n",
    "        drops_2.append(feature)\n",
    "len(drops_2) # 348 variables removed # 2 variables preserved\n",
    "\n",
    "drops_3 = []\n",
    "for feature in removables:\n",
    "    if removables[feature] >= 3:\n",
    "        drops_3.append(feature)\n",
    "len(drops_3) # 328 variables removed # 22 variables preserved\n",
    "\n",
    "drops_4 = []\n",
    "for feature in removables:\n",
    "    if removables[feature] >= 4:\n",
    "        drops_4.append(feature)\n",
    "len(drops_4) # 0 variables remnoved # 350 variables preserved \n",
    "\n",
    "### drops_3 selected to prevent underfitting or overfitting of the model\n",
    "xvals_filtered = xvals.drop(columns=drops_3)\n",
    "xvals_filtered.shape\n",
    "\n",
    "# xvals_filtered to be used as dependent variables used to build the model\n",
    "# yvals to be used as the response variable used to build the model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
